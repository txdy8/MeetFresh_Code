{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"data_processing.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOPyqE+n0f8yepnU0zsZXED"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rHrQKG9troxR"},"outputs":[],"source":["import requests \n","import json\n","import pandas as pd\n","import ntpath\n","import os\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from tqdm import tqdm\n","import re\n","%matplotlib inline"]},{"cell_type":"code","source":["ALL_COMPETITOR_DATA_PATH = '../data/competitor_list_BubbleTea_ShavedIce.csv'\n","ZIP_CODE = '../data/UsZipsList.csv'\n","COMPETITORS_COUNT_PATH = '../data/num_competitor.csv'\n","MF_COUNT_PATH = '../data/MF_NmOfComments_AvgNote.csv'\n","\n","ORIGINAL_COUNTY_DATA_PATH = '../data/county_data_num.csv'\n","GROUP2_DATA_PATH = '../data/group2/full_data_cleaned_RemoveNAN.csv'\n","MERGED_DATA_PATH = '../data/data_merged.csv'\n","MERGED_DATA_NOR_PATH = '../data/data_merged_nor.csv'"],"metadata":{"id":"meDa1l00vgwd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_zip_code(text):\n","    a = re.sub('^0', '', text)\n","    a = re.sub('\\.0', '', a)\n","    a = re.sub('... ...', '0', a)\n","    a = re.sub('[A-Z].[A-Z]', '0', a)\n","    return re.sub('. .', '0', a)"],"metadata":{"id":"5EOT6bz-ynuZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_competitors_count():\n","  resto = pd.read_csv(ALL_COMPETITOR_DATA_PATH)\n","\n","  num_res_state = resto.groupby('state').agg({'id':'count', 'rating':'mean'}).sort_values('id', ascending=False)\n","  num_res_state.reset_index(inplace=True)\n","\n","  num_res_city = resto.groupby('city').agg({'id':'count'}).sort_values('id', ascending=False)\n","  num_res_city.reset_index(inplace=True)\n","\n","  zipcode = pd.read_csv(ZIP_CODE)\n","\n","  num_res_zipcode = resto.groupby('zip code').agg({'id':'count'}).sort_values('id', ascending=False)\n","  num_res_zipcode.reset_index(inplace=True)\n","  num_res_zipcode['zip code'] = num_res_zipcode['zip code'].apply(clean_zip_code).astype('int64')\n","\n","  num_res_zipcode['county'] = 0\n","  num_res_zipcode['city'] = 0\n","  num_res_zipcode['state'] = 0\n","  num_res_zipcode['population'] = 0\n","\n","  for i in tqdm(range(num_res_zipcode.shape[0])):\n","  for j in range(zipcode.shape[0]):\n","    if num_res_zipcode.loc[i,'zip code'] == zipcode.loc[j,'zip']:\n","      num_res_zipcode.loc[i,'county'] = zipcode.loc[j,'county_name']\n","      num_res_zipcode.loc[i,'city'] = zipcode.loc[j,'city']\n","      num_res_zipcode.loc[i,'state'] = zipcode.loc[j,'state_name']\n","      num_res_zipcode.loc[i,'population'] = zipcode.loc[j,'population']\n","      continue\n","\n","  num_res_zipcode = num_res_zipcode.rename(columns = {'id':'num_competitor'})\n","  \n","  num_res_zipcode.to_csv(COMPETITORS_COUNT_PATH)"],"metadata":{"id":"JaUIc20Tx9UK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_MF_count():\n","  resto = pd.read_csv(ALL_COMPETITOR_DATA_PATH, index_col=0)\n","  resto = resto[resto['name']=='Meet Fresh']\n","\n","  resto['zip code'] = resto['zip code'].apply(clean_zip_code).astype('int64')\n","  resto = resto.drop(resto[resto['zip code']==0].index)\n","\n","  # add fips info\n","  zipcode = pd.read_csv(ZIP_CODE)\n","  data = pd.merge(resto,zipcode, how = 'left', left_on=['zip code'], right_on = ['zip']).set_index('county_fips').drop_duplicates(subset='id')\n","\n","  data = data.groupby('county_fips').agg({\"review count\":\"sum\", \"rating\":\"mean\"})\n","  data.to_csv(MF_COUNT_PATH)"],"metadata":{"id":"EPhOqVOSv8bY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def combien_data_group2():\n","  # 处理state id\n","  group_2 = pd.read_csv(GROUP2_DATA_PATH)\n","  pd.unique(group_2['state_id'])\n","  values = ['PR', 'VI', 'AK', 'HI']\n","  group_2 = group_2[group_2.state_id.isin(values) == False]\n","  pd.unique(group_2['state_id'])\n","  group_2['state_id'] = group_2['state_id'].replace(['MA', 'RI', 'NH', 'ME', 'VT', 'CT', 'NY', 'NJ', 'PA', 'DE', 'DC',\n","                                                   'VA', 'MD', 'WV', 'NC', 'SC', 'GA', 'FL', 'AL', 'TN', 'MS', 'KY',\n","                                                   'OH', 'IN', 'MI', 'IA', 'WI', 'MN', 'SD', 'ND', 'MT', 'IL', 'MO',\n","                                                   'KS', 'NE', 'LA', 'AR', 'OK', 'TX', 'CO', 'WY', 'ID', 'UT', 'AZ',\n","                                                   'NM', 'NV', 'CA', 'OR', 'WA'],\n","                                                  ['Massachusetts', 'Rhode Island', 'New Hampshire', 'Maine', 'Vermont',\n","                                                   'Connecticut', 'New York', 'New Jersey', 'Pennsylvania', 'Delaware',\n","                                                   'District of Columbia', 'Virginia', 'Maryland', 'West Virginia',\n","                                                   'North Carolina', 'South Carolina', 'Georgia', 'Florida', 'Alabama',\n","                                                   'Tennessee', 'Mississippi', 'Kentucky', 'Ohio', 'Indiana', 'Michigan',\n","                                                   'Iowa', 'Wisconsin', 'Minnesota', 'South Dakota', 'North Dakota',\n","                                                   'Montana', 'Illinois', 'Missouri', 'Kansas', 'Nebraska', 'Louisiana',\n","                                                   'Arkansas', 'Oklahoma', 'Texas', 'Colorado', 'Wyoming', 'Idaho', 'Utah',\n","                                                   'Arizona', 'New Mexico', 'Nevada', 'California', 'Oregon', 'Washington']\n","                                                  )\n","  \n","  zipcode = pd.read_csv(ZIP_CODE, index_col = 'zip')\n","\n","  old = group_2.set_index('zip')\n","  old['county_fips'] = zipcode['county_fips']\n","  data = old.loc[old['county_fips'].notnull()]\n","\n","  groupby = data.groupby(['state_id','county_name']).agg('mean').reset_index()\n","  groupby['county_fips'] = groupby['county_fips'].astype('int')\n","  groupby.rename(columns = {'county_fips':'FIPS'}, inplace=True)\n","  groupby = groupby.set_index('FIPS')\n","\n","  count = data.groupby(['state_id','county_name']).agg('count')\n","  count.index=groupby.index\n","\n","  data = groupby*count\n","\n","  original_data = pd.read_csv(ORIGINAL_COUNTY_DATA_PATH, index_col = 'FIPS').drop(columns='Unnamed: 0', inplace=True)\n","\n","  all = pd.merge(original_data, data, on='FIPS')\n","\n","  all.to_csv(MERGED_DATA_PATH)"],"metadata":{"id":"XKsLgmJN1wmb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def normalization():\n","  data = pd.read_csv(MERGED_DATA_PATH, index_col = 'FIPS')\n","  data_nor = data.groupby('State').transform(lambda x : (x-x.min())/(x.max()-x.min())).drop(columns = ['MF_exist'])\n","  header = data[['State', 'County', 'MF_exist']]\n","  result = pd.concat([header, data_nor], axis = 1)\n","\n","  result.to_csv(MERGED_DATA_NOR_PATH)"],"metadata":{"id":"NQf_YoGS3KKl"},"execution_count":null,"outputs":[]}]}